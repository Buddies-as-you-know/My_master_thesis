\chapter*{Abstract}
To address the challenge of bridging the gap between virtual and real-world environments in robotics, this study proposes an innovative approach that leverages Luma AI's Neural Radiance Fields (NeRF) and Unreal Engine 5 (UE5). The study uses standard smartphone cameras to capture video footage and efficiently create photorealistic 3D virtual environments. These environments are then integrated with UE5 to simulate realistic robotic navigation and manipulation tasks. The method's effectiveness is validated through objective evaluations using Navigation2 for operational efficacy and YOLO for accurate object recognition in both virtual and real settings. The study demonstrates a significant reduction in the time and resources required for environment creation compared to existing methods like NeRF2real and Matterport3D. This advancement facilitates simulation-driven development and testing in robotics and establishes a new standard for virtual environment generation. 
\thispagestyle{empty}
